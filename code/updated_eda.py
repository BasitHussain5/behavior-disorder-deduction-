# -*- coding: utf-8 -*-
"""updated-eda.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z6gCt5VRiSIx6rHwnPB9WVG5bVhitFsZ
"""

pip install wordcloud nltk matplotlib pandas

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import string
import re
import nltk
from tqdm import trange
from nltk import tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.probability import FreqDist
from collections import Counter
from sklearn.feature_extraction.text import CountVectorizer
from wordcloud import WordCloud
from nltk.corpus import stopwords
from wordcloud import STOPWORDS

file_path = '/kaggle/input/eda-dataset/cleanedDataset2.csv'
df = pd.read_csv(file_path)
df

df.describe()

df.info()

df.shape

df.isnull().sum()

df['sentence_length'] = df['Thought'].apply(len)

df

"""# Find the minimum and maximum sentence length and the corresponding sentence"""

min_length = df['sentence_length'].min()
min_sentence = df[df['sentence_length'] == min_length]['Thought'].iloc[0]

max_length = df['sentence_length'].max()
max_sentence = df[df['sentence_length'] == max_length]['Thought'].iloc[0]

print(f"Minimum sentence length: {min_length}")
print(f"Sentence with minimum length: {min_sentence}\n")
print(f"Maximum sentence length: {max_length}")
print(f"Sentence with maximum length: {max_sentence}")

# Count the frequency of each sentence length
length_counts = df['sentence_length'].value_counts().sort_index()

# Plot the bar chart
plt.figure(figsize=(10, 6))
length_counts.plot(kind='bar', color='skyblue')
# plt.title('Frequency of Sentence Lengths')
plt.xlabel('Sentence Length (in characters)')
plt.ylabel('Frequency')
plt.xticks(rotation=0)  # Rotate x-axis labels if needed
plt.show()

import matplotlib.pyplot as plt

# Plot histogram of sentence lengths
plt.figure(figsize=(12, 6))
plt.hist(df['sentence_length'], bins=30, color='skyblue', edgecolor='black')
# plt.title('Histogram of Sentence Lengths')
plt.xlabel('Sentence Length (in characters)')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt

# Plot line graph of sentence lengths
plt.figure(figsize=(12, 6))
plt.plot(df['sentence_number'], df['sentence_length'], marker='o', linestyle='-', color='skyblue')
# plt.title('Line Graph of Sentence Lengths by Sentence Number')
plt.xlabel('Sentence Number')
plt.ylabel('Sentence Length (in characters)')
plt.grid(True)
plt.show()







"""# Distribution of Labels"""

label_counts = df['Label'].value_counts()

# Create a pie chart
plt.figure(figsize=(4, 4))
plt.pie(label_counts, labels=label_counts.index, autopct='%1.1f%%', startangle=140)
# plt.title('Distribution of Labels')
plt.axis('equal')
plt.show()

"""## Exploratory Data Analysis

### Counts and Lenght:
Start by checking how long the reviews are
* Character count
* Word count
* Mean word length
* Mean sentence length
"""

lenght = len(str(df['Thought'][0]))
print(f'Length of a sample thought: {lenght}')

df['Length'] = df['Thought'].str.len()
df.head(10)

"""#### **Word Count**: Number of words in a thought"""

word_count = str(df['Thought'][0]).split()
print(f'Word count in a sample thought: {len(word_count)}')

def word_count(review):
    review_list = review.split()
    return len(review_list)

df['Word_count'] = df['Thought'].apply(word_count)
df.head(10)

"""#### **Mean word length**: Average length of words"""

df['mean_word_length'] = df['Thought'].map(lambda rev: np.mean([len(word) for word in rev.split()]))
df.head(10)

"""#### **Mean sentence length**: Average length of the sentences in the review"""

np.mean([len(sent) for sent in tokenize.sent_tokenize(df['Thought'][0])])

df['mean_sent_length'] = df['Thought'].map(lambda rev: np.mean([len(sent) for sent in tokenize.sent_tokenize(rev)]))
df.head(10)

def visualize(df, col):
    plt.figure(figsize=(14, 6))

    plt.subplot(1, 2, 1)
    sns.boxplot(x='Label', y=col, data=df)
    plt.xlabel('Label', labelpad=12.5)
    plt.ylabel(col, labelpad=12.5)
    plt.title(f'Boxplot of {col} by Label')

    plt.subplot(1, 2, 2)
    sns.kdeplot(data=df, x=col, hue='Label', common_norm=False)
    plt.xlabel(col, labelpad=12.5)
    plt.ylabel('Density', labelpad=12.5)
    plt.title(f'Distribution of {col} by Label')

    plt.tight_layout()
    plt.show()

features = ['Length', 'Word_count', 'mean_word_length', 'mean_sent_length']
for feature in features:
    visualize(df, feature)

"""## Term Frequency Analysis
Examining the most frequently occuring words is one of the most popular systems of Text analytics. For example, in a sentiment analysis problem, a positive text is bound to have words like 'good', 'great', 'nice', etc. more in number than other words that imply otherwise.

*Note*: Term Frequencies are more than counts and lenghts, so the first requirement is to preprocess the text
"""

df = df.drop(features, axis=1)
df.head()

"""There is no missing data, therefore, we can move to the next stage. For Term frequency analysis, it is essential that the text data be preprocessed.
* Lowercase
* Remove punctutations
* Stopword removal
"""

print(df.head(10))

df['Thought'][0]

def corpus(text):
    text_list = text.split()
    return text_list

df['Review_lists'] = df['Thought'].apply(corpus)
df.head(10)

corpus = []
for i in trange(df.shape[0], ncols=150, nrows=10, colour='green', smoothing=0.8):
    corpus += df['Review_lists'][i]
len(corpus)

mostCommon = Counter(corpus).most_common(10)
mostCommon

words = []
freq = []
for word, count in mostCommon:
    words.append(word)
    freq.append(count)

sns.barplot(x=freq, y=words)
# plt.title('Top 10 Most Frequently Occuring Words')
plt.show()

"""## Most Frequently occuring N_grams

**What is an N-gram?** <br>
An n-gram is sequence of n words in a text. Most words by themselves may not present the entire context. Typically adverbs such as 'most' or 'very' are used to modify verbs and adjectives. Therefore, n-grams help analyse phrases and not just words which can lead to better insights.
<br>
> A **Bi-gram** means two words in a sequence. 'Very good' or 'Too great'<br>
> A **Tri-gram** means three words in a sequence. 'How was your day' would be broken down to 'How was your' and 'was your day'.<br>

For separating text into n-grams, we will use `CountVectorizer` from Sklearn
"""

cv = CountVectorizer(ngram_range=(2,2))
bigrams = cv.fit_transform(df['Thought'])

count_values = bigrams.toarray().sum(axis=0)
ngram_freq = pd.DataFrame(sorted([(count_values[i], k) for k, i in cv.vocabulary_.items()], reverse = True))
ngram_freq.columns = ["frequency", "ngram"]

sns.barplot(x=ngram_freq['frequency'][:10], y=ngram_freq['ngram'][:10])
# plt.title('Top 10 Most Frequently Occuring Bigrams')
plt.show()

cv1 = CountVectorizer(ngram_range=(3,3))
trigrams = cv1.fit_transform(df['Thought'])
count_values = trigrams.toarray().sum(axis=0)
ngram_freq = pd.DataFrame(sorted([(count_values[i], k) for k, i in cv1.vocabulary_.items()], reverse = True))
ngram_freq.columns = ["frequency", "ngram"]

sns.barplot(x=ngram_freq['frequency'][:10], y=ngram_freq['ngram'][:10])
# plt.title('Top 10 Most Frequently Occuring Trigrams')
plt.show()

"""Word Clouds"""

def generate_wordcloud(label):
    text = " ".join(thought for thought in df[df['Label'] == label]['Thought'])
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    return wordcloud
labels = df['Label'].unique()
fig, axes = plt.subplots(1, len(labels), figsize=(20, 10))
for ax, label in zip(axes, labels):
    wordcloud = generate_wordcloud(label)
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.set_title(f"Word Cloud for {label} Thoughts")
    ax.axis('off')
plt.show()