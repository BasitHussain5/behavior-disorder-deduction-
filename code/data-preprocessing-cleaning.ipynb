{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9063597,"sourceType":"datasetVersion","datasetId":5465986},{"sourceId":9125062,"sourceType":"datasetVersion","datasetId":5508899}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install wordcloud nltk matplotlib pandas","metadata":{"execution":{"iopub.status.busy":"2024-08-15T11:22:23.157452Z","iopub.execute_input":"2024-08-15T11:22:23.157983Z","iopub.status.idle":"2024-08-15T11:22:37.388242Z","shell.execute_reply.started":"2024-08-15T11:22:23.157951Z","shell.execute_reply":"2024-08-15T11:22:37.386904Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wordcloud in /opt/conda/lib/python3.10/site-packages (1.9.3)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.5)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: numpy>=1.6.1 in /opt/conda/lib/python3.10/site-packages (from wordcloud) (1.26.4)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from wordcloud) (9.5.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.4)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport string\nimport re\nimport nltk\nfrom tqdm import trange\nfrom nltk import tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.probability import FreqDist\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nfrom wordcloud import STOPWORDS","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-15T11:22:37.390288Z","iopub.execute_input":"2024-08-15T11:22:37.390652Z","iopub.status.idle":"2024-08-15T11:22:37.397830Z","shell.execute_reply.started":"2024-08-15T11:22:37.390617Z","shell.execute_reply":"2024-08-15T11:22:37.396672Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"file_path = '/kaggle/input/dataset-to-identify-behavioral-disorder1/Dataset to Identify Behavioral Disorder/data_thought.csv'\ndf = pd.read_csv(file_path)\ndf","metadata":{"execution":{"iopub.status.busy":"2024-08-15T11:53:57.205498Z","iopub.execute_input":"2024-08-15T11:53:57.205870Z","iopub.status.idle":"2024-08-15T11:53:57.453791Z","shell.execute_reply.started":"2024-08-15T11:53:57.205839Z","shell.execute_reply":"2024-08-15T11:53:57.452764Z"},"trusted":true},"execution_count":66,"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"                                                  Thought     Label\n0                          Everyone is probably mad at me  negative\n1                          You're braver than you believe  positive\n2       When I open my eyes, you're all that I want to...  positive\n3       They clearly don’t want to be friends with me ...  negative\n4                              I've never done it before.  negative\n...                                                   ...       ...\n156780               I can't believe how fast time flies.  negative\n156781                            I can't eat spicy food.  negative\n156782                   Itâ€™s OK to SCREW Up! Nigel Raw  positive\n156783  Itâ€™s here. Itâ€™s beautiful. Itâ€™s signed. ...  positive\n156784             Stuck On You â€“ Cover by Alicia Widar  negative\n\n[156785 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Thought</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Everyone is probably mad at me</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>You're braver than you believe</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>When I open my eyes, you're all that I want to...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>They clearly don’t want to be friends with me ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I've never done it before.</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>156780</th>\n      <td>I can't believe how fast time flies.</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>156781</th>\n      <td>I can't eat spicy food.</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>156782</th>\n      <td>Itâ€™s OK to SCREW Up! Nigel Raw</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>156783</th>\n      <td>Itâ€™s here. Itâ€™s beautiful. Itâ€™s signed. ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>156784</th>\n      <td>Stuck On You â€“ Cover by Alicia Widar</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n<p>156785 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T11:32:01.145974Z","iopub.execute_input":"2024-08-15T11:32:01.146679Z","iopub.status.idle":"2024-08-15T11:32:01.307657Z","shell.execute_reply.started":"2024-08-15T11:32:01.146650Z","shell.execute_reply":"2024-08-15T11:32:01.306748Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"       Thought     Label\ncount   156785    156785\nunique  129051         2\ntop       Help  positive\nfreq        65     85677","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Thought</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>156785</td>\n      <td>156785</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>129051</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>Help</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>65</td>\n      <td>85677</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T11:32:01.748500Z","iopub.execute_input":"2024-08-15T11:32:01.748826Z","iopub.status.idle":"2024-08-15T11:32:01.789305Z","shell.execute_reply.started":"2024-08-15T11:32:01.748801Z","shell.execute_reply":"2024-08-15T11:32:01.788357Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 156785 entries, 0 to 156784\nData columns (total 2 columns):\n #   Column   Non-Null Count   Dtype \n---  ------   --------------   ----- \n 0   Thought  156785 non-null  object\n 1   Label    156785 non-null  object\ndtypes: object(2)\nmemory usage: 2.4+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-15T11:32:02.266425Z","iopub.execute_input":"2024-08-15T11:32:02.266783Z","iopub.status.idle":"2024-08-15T11:32:02.272703Z","shell.execute_reply.started":"2024-08-15T11:32:02.266754Z","shell.execute_reply":"2024-08-15T11:32:02.271855Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"(156785, 2)"},"metadata":{}}]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T11:32:02.739143Z","iopub.execute_input":"2024-08-15T11:32:02.739504Z","iopub.status.idle":"2024-08-15T11:32:02.778978Z","shell.execute_reply.started":"2024-08-15T11:32:02.739476Z","shell.execute_reply":"2024-08-15T11:32:02.778124Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"Thought    0\nLabel      0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df['Thought'] = df['Thought'].str.lower()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T11:54:35.143489Z","iopub.execute_input":"2024-08-15T11:54:35.143847Z","iopub.status.idle":"2024-08-15T11:54:35.219258Z","shell.execute_reply.started":"2024-08-15T11:54:35.143818Z","shell.execute_reply":"2024-08-15T11:54:35.218277Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"# word_count\ndf['word_count'] = df['Thought'].apply(lambda x: len(str(x).split()))\n# unique_word_count\ndf['unique_word_count'] = df['Thought'].apply(lambda x: len(set(str(x).split())))\n# stop_word_count\ndf['stop_word_count'] = df['Thought'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n# url_count\ndf['url_count'] = df['Thought'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n# mean_word_length\ndf['mean_word_length'] = df['Thought'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n# char_count\ndf['char_count'] = df['Thought'].apply(lambda x: len(str(x)))\n# punctuation_count\ndf['punctuation_count'] = df['Thought'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n# hashtag_count\ndf['hashtag_count'] = df['Thought'].apply(lambda x: len([c for c in str(x) if c == '#']))\n# mention_count\ndf['mention_count'] = df['Thought'].apply(lambda x: len([c for c in str(x) if c == '@']))\n# Length of sentence in characters\ndf['length_of_Thought'] = df['Thought'].apply(lambda x: len(str(x)))\ntotal_digits = (df['Thought'].str.findall(r'\\d').apply(len)).sum()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T11:54:36.533138Z","iopub.execute_input":"2024-08-15T11:54:36.533510Z","iopub.status.idle":"2024-08-15T11:54:44.748735Z","shell.execute_reply.started":"2024-08-15T11:54:36.533475Z","shell.execute_reply":"2024-08-15T11:54:44.747967Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-08-15T11:54:44.750487Z","iopub.execute_input":"2024-08-15T11:54:44.750830Z","iopub.status.idle":"2024-08-15T11:54:44.768467Z","shell.execute_reply.started":"2024-08-15T11:54:44.750799Z","shell.execute_reply":"2024-08-15T11:54:44.767487Z"},"trusted":true},"execution_count":69,"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"                                                  Thought     Label  \\\n0                          everyone is probably mad at me  negative   \n1                          you're braver than you believe  positive   \n2       when i open my eyes, you're all that i want to...  positive   \n3       they clearly don’t want to be friends with me ...  negative   \n4                              i've never done it before.  negative   \n...                                                   ...       ...   \n156780               i can't believe how fast time flies.  negative   \n156781                            i can't eat spicy food.  negative   \n156782                   itâ€™s ok to screw up! nigel raw  positive   \n156783  itâ€™s here. itâ€™s beautiful. itâ€™s signed. ...  positive   \n156784             stuck on you â€“ cover by alicia widar  negative   \n\n        word_count  unique_word_count  stop_word_count  url_count  \\\n0                6                  6                3          0   \n1                5                  5                3          0   \n2               12                 11                8          0   \n3               10                 10                5          0   \n4                5                  5                1          0   \n...            ...                ...              ...        ...   \n156780           7                  7                2          0   \n156781           5                  5                1          0   \n156782           7                  7                1          0   \n156783          10                  8                1          0   \n156784           8                  8                3          0   \n\n        mean_word_length  char_count  punctuation_count  hashtag_count  \\\n0               4.166667          30                  0              0   \n1               5.200000          30                  1              0   \n2               3.250000          50                  2              0   \n3               4.400000          53                  0              0   \n4               4.400000          26                  2              0   \n...                  ...         ...                ...            ...   \n156780          4.285714          36                  2              0   \n156781          3.800000          23                  2              0   \n156782          3.714286          32                  1              0   \n156783          5.400000          63                  4              0   \n156784          3.875000          38                  0              0   \n\n        mention_count  length_of_Thought  \n0                   0                 30  \n1                   0                 30  \n2                   0                 50  \n3                   0                 53  \n4                   0                 26  \n...               ...                ...  \n156780              0                 36  \n156781              0                 23  \n156782              0                 32  \n156783              0                 63  \n156784              0                 38  \n\n[156785 rows x 12 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Thought</th>\n      <th>Label</th>\n      <th>word_count</th>\n      <th>unique_word_count</th>\n      <th>stop_word_count</th>\n      <th>url_count</th>\n      <th>mean_word_length</th>\n      <th>char_count</th>\n      <th>punctuation_count</th>\n      <th>hashtag_count</th>\n      <th>mention_count</th>\n      <th>length_of_Thought</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>everyone is probably mad at me</td>\n      <td>negative</td>\n      <td>6</td>\n      <td>6</td>\n      <td>3</td>\n      <td>0</td>\n      <td>4.166667</td>\n      <td>30</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>you're braver than you believe</td>\n      <td>positive</td>\n      <td>5</td>\n      <td>5</td>\n      <td>3</td>\n      <td>0</td>\n      <td>5.200000</td>\n      <td>30</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>when i open my eyes, you're all that i want to...</td>\n      <td>positive</td>\n      <td>12</td>\n      <td>11</td>\n      <td>8</td>\n      <td>0</td>\n      <td>3.250000</td>\n      <td>50</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>they clearly don’t want to be friends with me ...</td>\n      <td>negative</td>\n      <td>10</td>\n      <td>10</td>\n      <td>5</td>\n      <td>0</td>\n      <td>4.400000</td>\n      <td>53</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>53</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>i've never done it before.</td>\n      <td>negative</td>\n      <td>5</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4.400000</td>\n      <td>26</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>156780</th>\n      <td>i can't believe how fast time flies.</td>\n      <td>negative</td>\n      <td>7</td>\n      <td>7</td>\n      <td>2</td>\n      <td>0</td>\n      <td>4.285714</td>\n      <td>36</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>156781</th>\n      <td>i can't eat spicy food.</td>\n      <td>negative</td>\n      <td>5</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3.800000</td>\n      <td>23</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>156782</th>\n      <td>itâ€™s ok to screw up! nigel raw</td>\n      <td>positive</td>\n      <td>7</td>\n      <td>7</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3.714286</td>\n      <td>32</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>156783</th>\n      <td>itâ€™s here. itâ€™s beautiful. itâ€™s signed. ...</td>\n      <td>positive</td>\n      <td>10</td>\n      <td>8</td>\n      <td>1</td>\n      <td>0</td>\n      <td>5.400000</td>\n      <td>63</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>63</td>\n    </tr>\n    <tr>\n      <th>156784</th>\n      <td>stuck on you â€“ cover by alicia widar</td>\n      <td>negative</td>\n      <td>8</td>\n      <td>8</td>\n      <td>3</td>\n      <td>0</td>\n      <td>3.875000</td>\n      <td>38</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>38</td>\n    </tr>\n  </tbody>\n</table>\n<p>156785 rows × 12 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(\"BEFORE\")\n\ntotal_word_count = df['word_count'].sum()\nprint(f\"Total Word Count: {total_word_count}\")\ntotal_unique_word_count = df['unique_word_count'].sum()\nprint(f\"Total Unique Word Count: {total_unique_word_count}\")\ntotal_stop_word_count = df['stop_word_count'].sum()\nprint(f\"Total Stop Word Count: {total_stop_word_count}\")\ntotal_url_count = df['url_count'].sum()\nprint(f\"Total URL Count: {total_url_count}\")\nmean_of_mean_word_length = df['mean_word_length'].mean()\nprint(f\"Mean of Mean Word Length: {mean_of_mean_word_length}\")\ntotal_char_count = df['char_count'].sum()\nprint(f\"Total Character Count: {total_char_count}\")\ntotal_punctuation_count = df['punctuation_count'].sum()\nprint(f\"Total Punctuation Count: {total_punctuation_count}\")\ntotal_hashtag_count = df['hashtag_count'].sum()\nprint(f\"Total Hashtag # Count: {total_hashtag_count}\")\ntotal_mention_count = df['mention_count'].sum()\nprint(f\"Total Mention @ Count: {total_mention_count}\")\nprint(f\"Total digits Count: {total_digits}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-15T11:54:44.769806Z","iopub.execute_input":"2024-08-15T11:54:44.770167Z","iopub.status.idle":"2024-08-15T11:54:44.782416Z","shell.execute_reply.started":"2024-08-15T11:54:44.770137Z","shell.execute_reply":"2024-08-15T11:54:44.781403Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"BEFORE\nTotal Word Count: 1798194\nTotal Unique Word Count: 1701532\nTotal Stop Word Count: 702087\nTotal URL Count: 198\nMean of Mean Word Length: 5.031666509446574\nTotal Character Count: 10541087\nTotal Punctuation Count: 267464\nTotal Hashtag # Count: 2176\nTotal Mention @ Count: 552\nTotal digits Count: 53549\n","output_type":"stream"}]},{"cell_type":"code","source":"# df['Thought'] = df['Thought'].str.lower()\ndef connections(text):\n    # Contractions\n    text = re.sub(r\"he's\", \"he is\", text)\n    text = re.sub(r\"there's\", \"there is\", text)\n    text = re.sub(r\"We're\", \"We are\", text)\n    text = re.sub(r\"That's\", \"That is\", text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"they're\", \"they are\", text)\n    text = re.sub(r\"Can't\", \"Cannot\", text)\n    text = re.sub(r\"wasn't\", \"was not\", text)\n    text = re.sub(r\"don\\x89Ûªt\", \"do not\", text)\n    text = re.sub(r\"aren't\", \"are not\", text)\n    text = re.sub(r\"isn't\", \"is not\", text)\n    text = re.sub(r\"What's\", \"What is\", text)\n    text = re.sub(r\"haven't\", \"have not\", text)\n    text = re.sub(r\"hasn't\", \"has not\", text)\n    text = re.sub(r\"There's\", \"There is\", text)\n    text = re.sub(r\"He's\", \"He is\", text)\n    text = re.sub(r\"It's\", \"It is\", text)\n    text = re.sub(r\"you're\", \"you are\", text)\n    text = re.sub(r\"i'm\", \"i am\", text)\n    text = re.sub(r\"shouldn't\", \"should not\", text)\n    text = re.sub(r\"wouldn't\", \"would not\", text)\n    text = re.sub(r\"i'm\", \"I am\", text)\n    text = re.sub(r\"I\\x89Ûªm\", \"I am\", text)\n    text = re.sub(r\"I'm\", \"I am\", text)\n    text = re.sub(r\"isn't\", \"is not\", text)\n    text = re.sub(r\"here's\", \"here is\", text)\n    text = re.sub(r\"you've\", \"you have\", text)\n    text = re.sub(r\"you\\x89Ûªve\", \"you have\", text)\n    text = re.sub(r\"we're\", \"we are\", text)\n    text = re.sub(r\"what's\", \"what is\", text)\n    text = re.sub(r\"couldn't\", \"could not\", text)\n    text = re.sub(r\"we've\", \"we have\", text)\n    text = re.sub(r\"it\\x89Ûªs\", \"it is\", text)\n    text = re.sub(r\"doesn\\x89Ûªt\", \"does not\", text)\n    text = re.sub(r\"It\\x89Ûªs\", \"It is\", text)\n    text = re.sub(r\"Here\\x89Ûªs\", \"Here is\", text)\n    text = re.sub(r\"who's\", \"who is\", text)\n    text = re.sub(r\"I\\x89Ûªve\", \"I have\", text)\n    text = re.sub(r\"y'all\", \"you all\", text)\n    text = re.sub(r\"can\\x89Ûªt\", \"cannot\", text)\n    text = re.sub(r\"would've\", \"would have\", text)\n    text = re.sub(r\"it'll\", \"it will\", text)\n    text = re.sub(r\"we'll\", \"we will\", text)\n    text = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", text)\n    text = re.sub(r\"We've\", \"We have\", text)\n    text = re.sub(r\"he'll\", \"he will\", text)\n    text = re.sub(r\"Y'all\", \"You all\", text)\n    text = re.sub(r\"Weren't\", \"Were not\", text)\n    text = re.sub(r\"Didn't\", \"Did not\", text)\n    text = re.sub(r\"they'll\", \"they will\", text)\n    text = re.sub(r\"they'd\", \"they would\", text)\n    text = re.sub(r\"DON'T\", \"DO NOT\", text)\n    text = re.sub(r\"that\\x89Ûªs\", \"That is\", text)\n    text = re.sub(r\"they've\", \"they have\", text)\n    text = re.sub(r\"i'd\", \"I would\", text)\n    text = re.sub(r\"should've\", \"should have\", text)\n    text = re.sub(r\"you\\x89Ûªre\", \"You are\", text)\n    text = re.sub(r\"You're\", \"You are\", text)\n    text = re.sub(r\"you're\", \"You are\", text)\n    text = re.sub(r\"youre\", \"you are\", text)\n    text = re.sub(r\"ur\", \"you are\", text)\n    text = re.sub(r\"where's\", \"where is\", text)\n    text = re.sub(r\"don\\x89Ûªt\", \"Do not\", text)\n    text = re.sub(r\"we'd\", \"we would\", text)\n    text = re.sub(r\"i'll\", \"I will\", text)\n    text = re.sub(r\"weren't\", \"were not\", text)\n    text = re.sub(r\"they're\", \"They are\", text)\n    text = re.sub(r\"can\\x89Ûªt\", \"Can not\", text)\n    text = re.sub(r\"you\\x89Ûªll\", \"you will\", text)\n    text = re.sub(r\"I\\x89Ûªd\", \"I would\", text)\n    text = re.sub(r\"let's\", \"let us\", text)\n    text = re.sub(r\"it's\", \"it is\", text)\n    text = re.sub(r\"can't\", \"can not\", text)\n    text = re.sub(r\"don't\", \"do not\", text)\n    text = re.sub(r\"dont\", \"do not\", text)\n    text = re.sub(r\"you're\", \"you are\", text)\n    text = re.sub(r\"i've\", \"I have\", text)\n    text = re.sub(r\"that's\", \"that is\", text)\n    text = re.sub(r\"i'll\", \"I will\", text)\n    text = re.sub(r\"doesn't\", \"does not\", text)\n    text = re.sub(r\"i'd\", \"I would\", text)\n    text = re.sub(r\"didn't\", \"did not\", text)\n    text = re.sub(r\"ain't\", \"am not\", text)\n    text = re.sub(r\"you'll\", \"you will\", text)\n    text = re.sub(r\"I've\", \"I have\", text)\n    text = re.sub(r\"don't\", \"do not\", text)\n    text = re.sub(r\"I'll\", \"I will\", text)\n    text = re.sub(r\"I'd\", \"I would\", text)\n    text = re.sub(r\"let's\", \"Let us\", text)\n    text = re.sub(r\"you'd\", \"You would\", text)\n    text = re.sub(r\"It's\", \"It is\", text)\n    text = re.sub(r\"Ain't\", \"am not\", text)\n    text = re.sub(r\"haven't\", \"Have not\", text)\n    text = re.sub(r\"could've\", \"Could have\", text)\n    text = re.sub(r\"youve\", \"you have\", text)  \n    text = re.sub(r\"donå«t\", \"do not\", text)\n    text = re.sub(r\"ain't\", \"is not\", text)\n    text = re.sub(r\"could've\", \"could have\", text)\n    text = re.sub(r\"might've\", \"might have\", text)\n    text = re.sub(r\"must've\", \"must have\", text)\n    text = re.sub(r\"should've\", \"should have\", text)\n    text = re.sub(r\"would've\", \"would have\", text)\n    text = re.sub(r\"they'd\", \"they would\", text)  # or \"they had\" based on context\n    text = re.sub(r\"they'd've\", \"they would have\", text)\n    text = re.sub(r\"you'd\", \"you would\", text)  # or \"you had\" based on context\n    text = re.sub(r\"you'd've\", \"you would have\", text)\n    text = re.sub(r\"I'd've\", \"I would have\", text)\n    text = re.sub(r\"he'd\", \"he would\", text)  # or \"he had\" based on context\n    text = re.sub(r\"he'd've\", \"he would have\", text)\n    text = re.sub(r\"she'd\", \"she would\", text)  # or \"she had\" based on context\n    text = re.sub(r\"she'd've\", \"she would have\", text)\n    text = re.sub(r\"we'd've\", \"we would have\", text)\n    text = re.sub(r\"they'd've\", \"they would have\", text)\n    return text\ndef remove_punctuation(text):\n    return text.translate(str.maketrans('', '', string.punctuation))\ndef remove_urls(text):\n    text = re.sub(r'http[s]?://\\S+', '', text)\n    text = re.sub(r'http\\w+', '', text)\n    text = re.sub(r'https\\w+', '', text)\n    text = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", text)\n    return text\ndef remove_sc(text):\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = re.sub(r\"\\x89Û_\", \"\", text)\n    text = re.sub(r\"\\x89ÛÒ\", \"\", text)\n    text = re.sub(r\"\\x89ÛÓ\", \"\", text)\n    text = re.sub(r\"\\x89ÛÏWhen\", \"When\", text)\n    text = re.sub(r\"\\x89ÛÏ\", \"\", text)\n    text = re.sub(r\"China\\x89Ûªs\", \"China's\", text)\n    text = re.sub(r\"let\\x89Ûªs\", \"let's\", text)\n    text = re.sub(r\"\\x89Û÷\", \"\", text)\n    text = re.sub(r\"\\x89Ûª\", \"\", text)\n    text = re.sub(r\"\\x89Û\\x9d\", \"\", text)\n    text = re.sub(r\"å_\", \"\", text)\n    text = re.sub(r\"\\x89Û¢\", \"\", text)\n    text = re.sub(r\"\\x89Û¢åÊ\", \"\", text)\n    text = re.sub(r\"fromåÊwounds\", \"from wounds\", text)\n    text = re.sub(r\"åÊ\", \"\", text)\n    text = re.sub(r\"åÈ\", \"\", text)\n    text = re.sub(r\"JapÌ_n\", \"Japan\", text)    \n    text = re.sub(r\"Ì©\", \"e\", text)\n    text = re.sub(r\"å¨\", \"\", text)\n    text = re.sub(r\"SuruÌ¤\", \"Suruc\", text)\n    text = re.sub(r\"åÇ\", \"\", text)\n    text = re.sub(r\"å£3million\", \"3 million\", text)\n    text = re.sub(r\"åÀ\", \"\", text)\n    return text\n\ndef remove_hashtags_mentions(text):\n    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n    return text\ndef remove_stopwords(text):\n    return ' '.join([word for word in text.split() if word.lower() not in STOPWORDS])\ndef remove_slang(text):\n    # Typos, slang and informal abbreviations\n    text = re.sub(r\"w/e\", \"whatever\", text)\n    text = re.sub(r\"w/\", \"with\", text)\n    text = re.sub(r\"USAgov\", \"USA government\", text)\n    text = re.sub(r\"recentlu\", \"recently\", text)\n    text = re.sub(r\"Ph0tos\", \"Photos\", text)\n    text = re.sub(r\"amirite\", \"am I right\", text)\n    text = re.sub(r\"exp0sed\", \"exposed\", text)\n    text = re.sub(r\"<3\", \"love\", text)\n    text = re.sub(r\"amageddon\", \"armageddon\", text)\n    text = re.sub(r\"Trfc\", \"Traffic\", text)\n    text = re.sub(r\"8/5/2015\", \"2015-08-05\", text)\n    text = re.sub(r\"WindStorm\", \"Wind Storm\", text)\n    text = re.sub(r\"8/6/2015\", \"2015-08-06\", text)\n    text = re.sub(r\"10:38PM\", \"10:38 PM\", text)\n    text = re.sub(r\"10:30pm\", \"10:30 PM\", text)\n    text = re.sub(r\"16yr\", \"16 year\", text)\n    text = re.sub(r\"lmao\", \"laughing my ass off\", text)\n    text = re.sub(r\"TRAUMATISED\", \"traumatized\", text)\n    return text\n\ndef entity(text):\n    # Character entity references\n    text = re.sub(r\"&gt;\", \">\", text)\n    text = re.sub(r\"&lt;\", \"<\", text)\n    text = re.sub(r\"&amp;\", \"&\", text)\n    return text\n\ndef remove_excessive_whitespace(text):\n    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n    text = text.strip()  # Remove leading and trailing spaces\n    return text\n\n \n\n# Apply the functions\ndf['Thought'] = df['Thought'].apply(connections)\ndf['Thought'] = df['Thought'].apply(remove_stopwords)\ndf['Thought'] = df['Thought'].apply(remove_slang)\ndf['Thought'] = df['Thought'].apply(entity)\ndf['Thought'] = df['Thought'].apply(remove_urls)\ndf['Thought'] = df['Thought'].apply(remove_hashtags_mentions)\ndf['Thought'] = df['Thought'].apply(remove_sc)\ndf['Thought'] = df['Thought'].apply(remove_punctuation)\ndf['Thought'] = df['Thought'].apply(remove_excessive_whitespace)\ndf['Thought'] = df['Thought'].str.replace(r'\\d+', '', regex=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T11:54:44.785581Z","iopub.execute_input":"2024-08-15T11:54:44.786031Z","iopub.status.idle":"2024-08-15T11:55:22.264921Z","shell.execute_reply.started":"2024-08-15T11:54:44.785982Z","shell.execute_reply":"2024-08-15T11:55:22.263893Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-08-15T11:55:22.266122Z","iopub.execute_input":"2024-08-15T11:55:22.266408Z","iopub.status.idle":"2024-08-15T11:55:22.282822Z","shell.execute_reply.started":"2024-08-15T11:55:22.266383Z","shell.execute_reply":"2024-08-15T11:55:22.281889Z"},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"                                                  Thought     Label  \\\n0                                   everyone probably mad  negative   \n1                                          braver believe  positive   \n2                                      open eyes want see  positive   \n3                       clearly dont want friends anymore  negative   \n4                                       never done before  negative   \n...                                                   ...       ...   \n156780                            believe fast time flies  negative   \n156781                                     eat spicy food  negative   \n156782                         itâs ok screw up nigel raw  positive   \n156783  itâs here itâs beautiful itâs signed fits righ...  positive   \n156784                         stuck â cover alicia widar  negative   \n\n        word_count  unique_word_count  stop_word_count  url_count  \\\n0                6                  6                3          0   \n1                5                  5                3          0   \n2               12                 11                8          0   \n3               10                 10                5          0   \n4                5                  5                1          0   \n...            ...                ...              ...        ...   \n156780           7                  7                2          0   \n156781           5                  5                1          0   \n156782           7                  7                1          0   \n156783          10                  8                1          0   \n156784           8                  8                3          0   \n\n        mean_word_length  char_count  punctuation_count  hashtag_count  \\\n0               4.166667          30                  0              0   \n1               5.200000          30                  1              0   \n2               3.250000          50                  2              0   \n3               4.400000          53                  0              0   \n4               4.400000          26                  2              0   \n...                  ...         ...                ...            ...   \n156780          4.285714          36                  2              0   \n156781          3.800000          23                  2              0   \n156782          3.714286          32                  1              0   \n156783          5.400000          63                  4              0   \n156784          3.875000          38                  0              0   \n\n        mention_count  length_of_Thought  \n0                   0                 30  \n1                   0                 30  \n2                   0                 50  \n3                   0                 53  \n4                   0                 26  \n...               ...                ...  \n156780              0                 36  \n156781              0                 23  \n156782              0                 32  \n156783              0                 63  \n156784              0                 38  \n\n[156785 rows x 12 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Thought</th>\n      <th>Label</th>\n      <th>word_count</th>\n      <th>unique_word_count</th>\n      <th>stop_word_count</th>\n      <th>url_count</th>\n      <th>mean_word_length</th>\n      <th>char_count</th>\n      <th>punctuation_count</th>\n      <th>hashtag_count</th>\n      <th>mention_count</th>\n      <th>length_of_Thought</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>everyone probably mad</td>\n      <td>negative</td>\n      <td>6</td>\n      <td>6</td>\n      <td>3</td>\n      <td>0</td>\n      <td>4.166667</td>\n      <td>30</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>braver believe</td>\n      <td>positive</td>\n      <td>5</td>\n      <td>5</td>\n      <td>3</td>\n      <td>0</td>\n      <td>5.200000</td>\n      <td>30</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>open eyes want see</td>\n      <td>positive</td>\n      <td>12</td>\n      <td>11</td>\n      <td>8</td>\n      <td>0</td>\n      <td>3.250000</td>\n      <td>50</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>clearly dont want friends anymore</td>\n      <td>negative</td>\n      <td>10</td>\n      <td>10</td>\n      <td>5</td>\n      <td>0</td>\n      <td>4.400000</td>\n      <td>53</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>53</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>never done before</td>\n      <td>negative</td>\n      <td>5</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4.400000</td>\n      <td>26</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>156780</th>\n      <td>believe fast time flies</td>\n      <td>negative</td>\n      <td>7</td>\n      <td>7</td>\n      <td>2</td>\n      <td>0</td>\n      <td>4.285714</td>\n      <td>36</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>156781</th>\n      <td>eat spicy food</td>\n      <td>negative</td>\n      <td>5</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3.800000</td>\n      <td>23</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>156782</th>\n      <td>itâs ok screw up nigel raw</td>\n      <td>positive</td>\n      <td>7</td>\n      <td>7</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3.714286</td>\n      <td>32</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>156783</th>\n      <td>itâs here itâs beautiful itâs signed fits righ...</td>\n      <td>positive</td>\n      <td>10</td>\n      <td>8</td>\n      <td>1</td>\n      <td>0</td>\n      <td>5.400000</td>\n      <td>63</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>63</td>\n    </tr>\n    <tr>\n      <th>156784</th>\n      <td>stuck â cover alicia widar</td>\n      <td>negative</td>\n      <td>8</td>\n      <td>8</td>\n      <td>3</td>\n      <td>0</td>\n      <td>3.875000</td>\n      <td>38</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>38</td>\n    </tr>\n  </tbody>\n</table>\n<p>156785 rows × 12 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"nan_counts_per_column = df.isna().sum()\nprint(\"NaN counts per column:\")\nprint(nan_counts_per_column)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T11:55:22.284029Z","iopub.execute_input":"2024-08-15T11:55:22.284349Z","iopub.status.idle":"2024-08-15T11:55:22.326559Z","shell.execute_reply.started":"2024-08-15T11:55:22.284320Z","shell.execute_reply":"2024-08-15T11:55:22.325348Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"NaN counts per column:\nThought              0\nLabel                0\nword_count           0\nunique_word_count    0\nstop_word_count      0\nurl_count            0\nmean_word_length     0\nchar_count           0\npunctuation_count    0\nhashtag_count        0\nmention_count        0\nlength_of_Thought    0\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"nan_count = df['Thought'].isna().sum()\n\nprint(nan_count)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T11:55:22.327925Z","iopub.execute_input":"2024-08-15T11:55:22.328327Z","iopub.status.idle":"2024-08-15T11:55:22.354277Z","shell.execute_reply.started":"2024-08-15T11:55:22.328293Z","shell.execute_reply":"2024-08-15T11:55:22.353340Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"0\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# word_count\ndf['word_count'] = df['Thought'].apply(lambda x: len(str(x).split()))\n# unique_word_count\ndf['unique_word_count'] = df['Thought'].apply(lambda x: len(set(str(x).split())))\n# stop_word_count\ndf['stop_word_count'] = df['Thought'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n# url_count\ndf['url_count'] = df['Thought'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n# mean_word_length\ndf['mean_word_length'] = df['Thought'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n# char_count\ndf['char_count'] = df['Thought'].apply(lambda x: len(str(x)))\n# punctuation_count\ndf['punctuation_count'] = df['Thought'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n# hashtag_count\ndf['hashtag_count'] = df['Thought'].apply(lambda x: len([c for c in str(x) if c == '#']))\n# mention_count\ndf['mention_count'] = df['Thought'].apply(lambda x: len([c for c in str(x) if c == '@']))\n# Length of sentence in characters\ndf['length_of_Thought'] = df['Thought'].apply(lambda x: len(str(x)))\ntotal_digits = (df['Thought'].str.findall(r'\\d').apply(len)).sum()\ndf['length_of_Thought'] = df['length_of_Thought'].astype(str)\ndf = df[df['length_of_Thought'].apply(len) > 1]","metadata":{"execution":{"iopub.status.busy":"2024-08-15T11:55:22.355366Z","iopub.execute_input":"2024-08-15T11:55:22.355633Z","iopub.status.idle":"2024-08-15T11:55:29.229766Z","shell.execute_reply.started":"2024-08-15T11:55:22.355610Z","shell.execute_reply":"2024-08-15T11:55:29.228936Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\n/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n  ret = ret.dtype.type(ret / rcount)\n","output_type":"stream"}]},{"cell_type":"code","source":"from nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\n# # Ensure STOPWORDS is defined\nSTOPWORDS = set(stopwords.words('english'))\n\n# Initialize stemmer and lemmatizer\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\ndef get_wordnet_pos(treebank_tag):\n    \"\"\"Convert treebank tags to WordNet tags.\"\"\"\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\ndef preprocess_text(text, use_stemming=False, use_lemmatization=False):\n    if use_stemming:\n        text = ' '.join([stemmer.stem(word) for word in text.split()])\n    if use_lemmatization:\n        pos_tags = pos_tag(text.split())\n        text = ' '.join([lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags])\n    return text\n\n# Apply preprocessing\ndf['Thought'] = df['Thought'].apply(lambda x: preprocess_text(x, use_stemming=True, use_lemmatization=False))","metadata":{"execution":{"iopub.status.busy":"2024-08-15T11:55:29.247263Z","iopub.execute_input":"2024-08-15T11:55:29.247518Z","iopub.status.idle":"2024-08-15T11:56:07.190512Z","shell.execute_reply.started":"2024-08-15T11:55:29.247496Z","shell.execute_reply":"2024-08-15T11:56:07.189483Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_33/2608468454.py:33: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['Thought'] = df['Thought'].apply(lambda x: preprocess_text(x, use_stemming=True, use_lemmatization=False))\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"AFTER\")\ntotal_word_count = df['word_count'].sum()\nprint(f\"Total Word Count: {total_word_count}\")\ntotal_unique_word_count = df['unique_word_count'].sum()\nprint(f\"Total Unique Word Count: {total_unique_word_count}\")\ntotal_stop_word_count = df['stop_word_count'].sum()\nprint(f\"Total Stop Word Count: {total_stop_word_count}\")\ntotal_url_count = df['url_count'].sum()\nprint(f\"Total URL Count: {total_url_count}\")\nmean_of_mean_word_length = df['mean_word_length'].mean()\nprint(f\"Mean of Mean Word Length: {mean_of_mean_word_length}\")\ntotal_char_count = df['char_count'].sum()\nprint(f\"Total Character Count: {total_char_count}\")\ntotal_punctuation_count = df['punctuation_count'].sum()\nprint(f\"Total Punctuation Count: {total_punctuation_count}\")\ntotal_hashtag_count = df['hashtag_count'].sum()\nprint(f\"Total Hashtag # Count: {total_hashtag_count}\")\ntotal_mention_count = df['mention_count'].sum()\nprint(f\"Total Mention @ Count: {total_mention_count}\")\nprint(f\"Total digits Count: {total_digits}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-15T11:55:29.231042Z","iopub.execute_input":"2024-08-15T11:55:29.231899Z","iopub.status.idle":"2024-08-15T11:55:29.244083Z","shell.execute_reply.started":"2024-08-15T11:55:29.231858Z","shell.execute_reply":"2024-08-15T11:55:29.243195Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"AFTER\nTotal Word Count: 1083478\nTotal Unique Word Count: 1061793\nTotal Stop Word Count: 20397\nTotal URL Count: 1\nMean of Mean Word Length: 6.183959909671485\nTotal Character Count: 7654188\nTotal Punctuation Count: 0\nTotal Hashtag # Count: 0\nTotal Mention @ Count: 0\nTotal digits Count: 0\n","output_type":"stream"}]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-08-15T11:56:07.191585Z","iopub.execute_input":"2024-08-15T11:56:07.191963Z","iopub.status.idle":"2024-08-15T11:56:07.209624Z","shell.execute_reply.started":"2024-08-15T11:56:07.191932Z","shell.execute_reply":"2024-08-15T11:56:07.208794Z"},"trusted":true},"execution_count":78,"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"                                          Thought     Label  word_count  \\\n0                             everyon probabl mad  negative           3   \n1                                   braver believ  positive           2   \n2                               open eye want see  positive           4   \n3                 clearli dont want friend anymor  negative           5   \n4                                never done befor  negative           3   \n...                                           ...       ...         ...   \n156780                       believ fast time fli  negative           4   \n156781                             eat spici food  negative           3   \n156782                  itâ ok screw up nigel raw  positive           6   \n156783  itâ here itâ beauti itâ sign fit right in  positive           9   \n156784                 stuck â cover alicia widar  negative           5   \n\n        unique_word_count  stop_word_count  url_count  mean_word_length  \\\n0                       3                0          0          6.333333   \n1                       2                0          0          6.500000   \n2                       4                0          0          3.750000   \n3                       5                0          0          5.800000   \n4                       3                1          0          5.000000   \n...                   ...              ...        ...               ...   \n156780                  4                0          0          5.000000   \n156781                  3                0          0          4.000000   \n156782                  6                1          0          3.500000   \n156783                  7                2          0          4.666667   \n156784                  5                0          0          4.400000   \n\n        char_count  punctuation_count  hashtag_count  mention_count  \\\n0               21                  0              0              0   \n1               14                  0              0              0   \n2               18                  0              0              0   \n3               33                  0              0              0   \n4               17                  0              0              0   \n...            ...                ...            ...            ...   \n156780          23                  0              0              0   \n156781          14                  0              0              0   \n156782          26                  0              0              0   \n156783          50                  0              0              0   \n156784          26                  0              0              0   \n\n       length_of_Thought  \n0                     21  \n1                     14  \n2                     18  \n3                     33  \n4                     17  \n...                  ...  \n156780                23  \n156781                14  \n156782                26  \n156783                50  \n156784                26  \n\n[151648 rows x 12 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Thought</th>\n      <th>Label</th>\n      <th>word_count</th>\n      <th>unique_word_count</th>\n      <th>stop_word_count</th>\n      <th>url_count</th>\n      <th>mean_word_length</th>\n      <th>char_count</th>\n      <th>punctuation_count</th>\n      <th>hashtag_count</th>\n      <th>mention_count</th>\n      <th>length_of_Thought</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>everyon probabl mad</td>\n      <td>negative</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6.333333</td>\n      <td>21</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>braver believ</td>\n      <td>positive</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6.500000</td>\n      <td>14</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>open eye want see</td>\n      <td>positive</td>\n      <td>4</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3.750000</td>\n      <td>18</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>clearli dont want friend anymor</td>\n      <td>negative</td>\n      <td>5</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5.800000</td>\n      <td>33</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>never done befor</td>\n      <td>negative</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>5.000000</td>\n      <td>17</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>156780</th>\n      <td>believ fast time fli</td>\n      <td>negative</td>\n      <td>4</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5.000000</td>\n      <td>23</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>156781</th>\n      <td>eat spici food</td>\n      <td>negative</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4.000000</td>\n      <td>14</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>156782</th>\n      <td>itâ ok screw up nigel raw</td>\n      <td>positive</td>\n      <td>6</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3.500000</td>\n      <td>26</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>156783</th>\n      <td>itâ here itâ beauti itâ sign fit right in</td>\n      <td>positive</td>\n      <td>9</td>\n      <td>7</td>\n      <td>2</td>\n      <td>0</td>\n      <td>4.666667</td>\n      <td>50</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>156784</th>\n      <td>stuck â cover alicia widar</td>\n      <td>negative</td>\n      <td>5</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4.400000</td>\n      <td>26</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>26</td>\n    </tr>\n  </tbody>\n</table>\n<p>151648 rows × 12 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T11:56:07.210632Z","iopub.execute_input":"2024-08-15T11:56:07.210889Z","iopub.status.idle":"2024-08-15T11:56:07.289222Z","shell.execute_reply.started":"2024-08-15T11:56:07.210867Z","shell.execute_reply":"2024-08-15T11:56:07.288284Z"},"trusted":true},"execution_count":79,"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"          word_count  unique_word_count  stop_word_count      url_count  \\\ncount  151648.000000      151648.000000    151648.000000  151648.000000   \nmean        7.144690           7.001695         0.134502       0.000007   \nstd         4.568663           4.266824         0.400050       0.002568   \nmin         1.000000           1.000000         0.000000       0.000000   \n25%         4.000000           4.000000         0.000000       0.000000   \n50%         6.000000           6.000000         0.000000       0.000000   \n75%         9.000000           8.000000         0.000000       0.000000   \nmax        90.000000          84.000000         8.000000       1.000000   \n\n       mean_word_length     char_count  punctuation_count  hashtag_count  \\\ncount     151648.000000  151648.000000           151648.0       151648.0   \nmean           6.183960      50.473386                0.0            0.0   \nstd            1.304269      33.421538                0.0            0.0   \nmin            1.857143      10.000000                0.0            0.0   \n25%            5.250000      27.000000                0.0            0.0   \n50%            6.000000      45.000000                0.0            0.0   \n75%            7.000000      64.000000                0.0            0.0   \nmax           41.000000     666.000000                0.0            0.0   \n\n       mention_count  \ncount       151648.0  \nmean             0.0  \nstd              0.0  \nmin              0.0  \n25%              0.0  \n50%              0.0  \n75%              0.0  \nmax              0.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word_count</th>\n      <th>unique_word_count</th>\n      <th>stop_word_count</th>\n      <th>url_count</th>\n      <th>mean_word_length</th>\n      <th>char_count</th>\n      <th>punctuation_count</th>\n      <th>hashtag_count</th>\n      <th>mention_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>151648.000000</td>\n      <td>151648.000000</td>\n      <td>151648.000000</td>\n      <td>151648.000000</td>\n      <td>151648.000000</td>\n      <td>151648.000000</td>\n      <td>151648.0</td>\n      <td>151648.0</td>\n      <td>151648.0</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>7.144690</td>\n      <td>7.001695</td>\n      <td>0.134502</td>\n      <td>0.000007</td>\n      <td>6.183960</td>\n      <td>50.473386</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>4.568663</td>\n      <td>4.266824</td>\n      <td>0.400050</td>\n      <td>0.002568</td>\n      <td>1.304269</td>\n      <td>33.421538</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.857143</td>\n      <td>10.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>4.000000</td>\n      <td>4.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.250000</td>\n      <td>27.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>6.000000</td>\n      <td>6.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>6.000000</td>\n      <td>45.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>9.000000</td>\n      <td>8.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>7.000000</td>\n      <td>64.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>90.000000</td>\n      <td>84.000000</td>\n      <td>8.000000</td>\n      <td>1.000000</td>\n      <td>41.000000</td>\n      <td>666.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T11:56:07.290311Z","iopub.execute_input":"2024-08-15T11:56:07.290589Z","iopub.status.idle":"2024-08-15T11:56:07.344183Z","shell.execute_reply.started":"2024-08-15T11:56:07.290565Z","shell.execute_reply":"2024-08-15T11:56:07.343403Z"},"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"Thought              0\nLabel                0\nword_count           0\nunique_word_count    0\nstop_word_count      0\nurl_count            0\nmean_word_length     0\nchar_count           0\npunctuation_count    0\nhashtag_count        0\nmention_count        0\nlength_of_Thought    0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df = df.dropna()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T11:56:07.345285Z","iopub.execute_input":"2024-08-15T11:56:07.345603Z","iopub.status.idle":"2024-08-15T11:56:07.427041Z","shell.execute_reply.started":"2024-08-15T11:56:07.345576Z","shell.execute_reply":"2024-08-15T11:56:07.426281Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"# Count total NaN values in the entire DataFrame\ntotal_nan_count = df.isna().sum().sum()\nprint(\"\\nTotal NaN values in the DataFrame:\")\nprint(total_nan_count)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T11:56:07.428278Z","iopub.execute_input":"2024-08-15T11:56:07.428540Z","iopub.status.idle":"2024-08-15T11:56:07.478523Z","shell.execute_reply.started":"2024-08-15T11:56:07.428517Z","shell.execute_reply":"2024-08-15T11:56:07.477665Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stdout","text":"\nTotal NaN values in the DataFrame:\n0\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}