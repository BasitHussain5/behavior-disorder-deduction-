# -*- coding: utf-8 -*-
"""data-preprocessing-cleaning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WfuabXlLEEvbQnDPOj0l00Mknv0R0zom
"""

pip install wordcloud nltk matplotlib pandas

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import string
import re
import nltk
from tqdm import trange
from nltk import tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.probability import FreqDist
from collections import Counter
from sklearn.feature_extraction.text import CountVectorizer
from wordcloud import WordCloud
from nltk.corpus import stopwords
from wordcloud import STOPWORDS

file_path = '/kaggle/input/dataset-to-identify-behavioral-disorder1/Dataset to Identify Behavioral Disorder/data_thought.csv'
df = pd.read_csv(file_path)
df

df.describe()

df.info()

df.shape

df.isnull().sum()

df['Thought'] = df['Thought'].str.lower()

# word_count
df['word_count'] = df['Thought'].apply(lambda x: len(str(x).split()))
# unique_word_count
df['unique_word_count'] = df['Thought'].apply(lambda x: len(set(str(x).split())))
# stop_word_count
df['stop_word_count'] = df['Thought'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))
# url_count
df['url_count'] = df['Thought'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))
# mean_word_length
df['mean_word_length'] = df['Thought'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))
# char_count
df['char_count'] = df['Thought'].apply(lambda x: len(str(x)))
# punctuation_count
df['punctuation_count'] = df['Thought'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))
# hashtag_count
df['hashtag_count'] = df['Thought'].apply(lambda x: len([c for c in str(x) if c == '#']))
# mention_count
df['mention_count'] = df['Thought'].apply(lambda x: len([c for c in str(x) if c == '@']))
# Length of sentence in characters
df['length_of_Thought'] = df['Thought'].apply(lambda x: len(str(x)))
total_digits = (df['Thought'].str.findall(r'\d').apply(len)).sum()

df

print("BEFORE")

total_word_count = df['word_count'].sum()
print(f"Total Word Count: {total_word_count}")
total_unique_word_count = df['unique_word_count'].sum()
print(f"Total Unique Word Count: {total_unique_word_count}")
total_stop_word_count = df['stop_word_count'].sum()
print(f"Total Stop Word Count: {total_stop_word_count}")
total_url_count = df['url_count'].sum()
print(f"Total URL Count: {total_url_count}")
mean_of_mean_word_length = df['mean_word_length'].mean()
print(f"Mean of Mean Word Length: {mean_of_mean_word_length}")
total_char_count = df['char_count'].sum()
print(f"Total Character Count: {total_char_count}")
total_punctuation_count = df['punctuation_count'].sum()
print(f"Total Punctuation Count: {total_punctuation_count}")
total_hashtag_count = df['hashtag_count'].sum()
print(f"Total Hashtag # Count: {total_hashtag_count}")
total_mention_count = df['mention_count'].sum()
print(f"Total Mention @ Count: {total_mention_count}")
print(f"Total digits Count: {total_digits}")

# df['Thought'] = df['Thought'].str.lower()
def connections(text):
    # Contractions
    text = re.sub(r"he's", "he is", text)
    text = re.sub(r"there's", "there is", text)
    text = re.sub(r"We're", "We are", text)
    text = re.sub(r"That's", "That is", text)
    text = re.sub(r"won't", "will not", text)
    text = re.sub(r"they're", "they are", text)
    text = re.sub(r"Can't", "Cannot", text)
    text = re.sub(r"wasn't", "was not", text)
    text = re.sub(r"don\x89Ûªt", "do not", text)
    text = re.sub(r"aren't", "are not", text)
    text = re.sub(r"isn't", "is not", text)
    text = re.sub(r"What's", "What is", text)
    text = re.sub(r"haven't", "have not", text)
    text = re.sub(r"hasn't", "has not", text)
    text = re.sub(r"There's", "There is", text)
    text = re.sub(r"He's", "He is", text)
    text = re.sub(r"It's", "It is", text)
    text = re.sub(r"you're", "you are", text)
    text = re.sub(r"i'm", "i am", text)
    text = re.sub(r"shouldn't", "should not", text)
    text = re.sub(r"wouldn't", "would not", text)
    text = re.sub(r"i'm", "I am", text)
    text = re.sub(r"I\x89Ûªm", "I am", text)
    text = re.sub(r"I'm", "I am", text)
    text = re.sub(r"isn't", "is not", text)
    text = re.sub(r"here's", "here is", text)
    text = re.sub(r"you've", "you have", text)
    text = re.sub(r"you\x89Ûªve", "you have", text)
    text = re.sub(r"we're", "we are", text)
    text = re.sub(r"what's", "what is", text)
    text = re.sub(r"couldn't", "could not", text)
    text = re.sub(r"we've", "we have", text)
    text = re.sub(r"it\x89Ûªs", "it is", text)
    text = re.sub(r"doesn\x89Ûªt", "does not", text)
    text = re.sub(r"It\x89Ûªs", "It is", text)
    text = re.sub(r"Here\x89Ûªs", "Here is", text)
    text = re.sub(r"who's", "who is", text)
    text = re.sub(r"I\x89Ûªve", "I have", text)
    text = re.sub(r"y'all", "you all", text)
    text = re.sub(r"can\x89Ûªt", "cannot", text)
    text = re.sub(r"would've", "would have", text)
    text = re.sub(r"it'll", "it will", text)
    text = re.sub(r"we'll", "we will", text)
    text = re.sub(r"wouldn\x89Ûªt", "would not", text)
    text = re.sub(r"We've", "We have", text)
    text = re.sub(r"he'll", "he will", text)
    text = re.sub(r"Y'all", "You all", text)
    text = re.sub(r"Weren't", "Were not", text)
    text = re.sub(r"Didn't", "Did not", text)
    text = re.sub(r"they'll", "they will", text)
    text = re.sub(r"they'd", "they would", text)
    text = re.sub(r"DON'T", "DO NOT", text)
    text = re.sub(r"that\x89Ûªs", "That is", text)
    text = re.sub(r"they've", "they have", text)
    text = re.sub(r"i'd", "I would", text)
    text = re.sub(r"should've", "should have", text)
    text = re.sub(r"you\x89Ûªre", "You are", text)
    text = re.sub(r"You're", "You are", text)
    text = re.sub(r"you're", "You are", text)
    text = re.sub(r"youre", "you are", text)
    text = re.sub(r"ur", "you are", text)
    text = re.sub(r"where's", "where is", text)
    text = re.sub(r"don\x89Ûªt", "Do not", text)
    text = re.sub(r"we'd", "we would", text)
    text = re.sub(r"i'll", "I will", text)
    text = re.sub(r"weren't", "were not", text)
    text = re.sub(r"they're", "They are", text)
    text = re.sub(r"can\x89Ûªt", "Can not", text)
    text = re.sub(r"you\x89Ûªll", "you will", text)
    text = re.sub(r"I\x89Ûªd", "I would", text)
    text = re.sub(r"let's", "let us", text)
    text = re.sub(r"it's", "it is", text)
    text = re.sub(r"can't", "can not", text)
    text = re.sub(r"don't", "do not", text)
    text = re.sub(r"dont", "do not", text)
    text = re.sub(r"you're", "you are", text)
    text = re.sub(r"i've", "I have", text)
    text = re.sub(r"that's", "that is", text)
    text = re.sub(r"i'll", "I will", text)
    text = re.sub(r"doesn't", "does not", text)
    text = re.sub(r"i'd", "I would", text)
    text = re.sub(r"didn't", "did not", text)
    text = re.sub(r"ain't", "am not", text)
    text = re.sub(r"you'll", "you will", text)
    text = re.sub(r"I've", "I have", text)
    text = re.sub(r"don't", "do not", text)
    text = re.sub(r"I'll", "I will", text)
    text = re.sub(r"I'd", "I would", text)
    text = re.sub(r"let's", "Let us", text)
    text = re.sub(r"you'd", "You would", text)
    text = re.sub(r"It's", "It is", text)
    text = re.sub(r"Ain't", "am not", text)
    text = re.sub(r"haven't", "Have not", text)
    text = re.sub(r"could've", "Could have", text)
    text = re.sub(r"youve", "you have", text)
    text = re.sub(r"donå«t", "do not", text)
    text = re.sub(r"ain't", "is not", text)
    text = re.sub(r"could've", "could have", text)
    text = re.sub(r"might've", "might have", text)
    text = re.sub(r"must've", "must have", text)
    text = re.sub(r"should've", "should have", text)
    text = re.sub(r"would've", "would have", text)
    text = re.sub(r"they'd", "they would", text)  # or "they had" based on context
    text = re.sub(r"they'd've", "they would have", text)
    text = re.sub(r"you'd", "you would", text)  # or "you had" based on context
    text = re.sub(r"you'd've", "you would have", text)
    text = re.sub(r"I'd've", "I would have", text)
    text = re.sub(r"he'd", "he would", text)  # or "he had" based on context
    text = re.sub(r"he'd've", "he would have", text)
    text = re.sub(r"she'd", "she would", text)  # or "she had" based on context
    text = re.sub(r"she'd've", "she would have", text)
    text = re.sub(r"we'd've", "we would have", text)
    text = re.sub(r"they'd've", "they would have", text)
    return text
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))
def remove_urls(text):
    text = re.sub(r'http[s]?://\S+', '', text)
    text = re.sub(r'http\w+', '', text)
    text = re.sub(r'https\w+', '', text)
    text = re.sub(r"https?:\/\/t.co\/[A-Za-z0-9]+", "", text)
    return text
def remove_sc(text):
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r"\x89Û_", "", text)
    text = re.sub(r"\x89ÛÒ", "", text)
    text = re.sub(r"\x89ÛÓ", "", text)
    text = re.sub(r"\x89ÛÏWhen", "When", text)
    text = re.sub(r"\x89ÛÏ", "", text)
    text = re.sub(r"China\x89Ûªs", "China's", text)
    text = re.sub(r"let\x89Ûªs", "let's", text)
    text = re.sub(r"\x89Û÷", "", text)
    text = re.sub(r"\x89Ûª", "", text)
    text = re.sub(r"\x89Û\x9d", "", text)
    text = re.sub(r"å_", "", text)
    text = re.sub(r"\x89Û¢", "", text)
    text = re.sub(r"\x89Û¢åÊ", "", text)
    text = re.sub(r"fromåÊwounds", "from wounds", text)
    text = re.sub(r"åÊ", "", text)
    text = re.sub(r"åÈ", "", text)
    text = re.sub(r"JapÌ_n", "Japan", text)
    text = re.sub(r"Ì©", "e", text)
    text = re.sub(r"å¨", "", text)
    text = re.sub(r"SuruÌ¤", "Suruc", text)
    text = re.sub(r"åÇ", "", text)
    text = re.sub(r"å£3million", "3 million", text)
    text = re.sub(r"åÀ", "", text)
    return text

def remove_hashtags_mentions(text):
    text = re.sub(r'#\w+', '', text)  # Remove hashtags
    text = re.sub(r'@\w+', '', text)  # Remove mentions
    return text
def remove_stopwords(text):
    return ' '.join([word for word in text.split() if word.lower() not in STOPWORDS])
def remove_slang(text):
    # Typos, slang and informal abbreviations
    text = re.sub(r"w/e", "whatever", text)
    text = re.sub(r"w/", "with", text)
    text = re.sub(r"USAgov", "USA government", text)
    text = re.sub(r"recentlu", "recently", text)
    text = re.sub(r"Ph0tos", "Photos", text)
    text = re.sub(r"amirite", "am I right", text)
    text = re.sub(r"exp0sed", "exposed", text)
    text = re.sub(r"<3", "love", text)
    text = re.sub(r"amageddon", "armageddon", text)
    text = re.sub(r"Trfc", "Traffic", text)
    text = re.sub(r"8/5/2015", "2015-08-05", text)
    text = re.sub(r"WindStorm", "Wind Storm", text)
    text = re.sub(r"8/6/2015", "2015-08-06", text)
    text = re.sub(r"10:38PM", "10:38 PM", text)
    text = re.sub(r"10:30pm", "10:30 PM", text)
    text = re.sub(r"16yr", "16 year", text)
    text = re.sub(r"lmao", "laughing my ass off", text)
    text = re.sub(r"TRAUMATISED", "traumatized", text)
    return text

def entity(text):
    # Character entity references
    text = re.sub(r"&gt;", ">", text)
    text = re.sub(r"&lt;", "<", text)
    text = re.sub(r"&amp;", "&", text)
    return text

def remove_excessive_whitespace(text):
    text = re.sub(r'\s+', ' ', text)  # Replace multiple spaces with a single space
    text = text.strip()  # Remove leading and trailing spaces
    return text



# Apply the functions
df['Thought'] = df['Thought'].apply(connections)
df['Thought'] = df['Thought'].apply(remove_stopwords)
df['Thought'] = df['Thought'].apply(remove_slang)
df['Thought'] = df['Thought'].apply(entity)
df['Thought'] = df['Thought'].apply(remove_urls)
df['Thought'] = df['Thought'].apply(remove_hashtags_mentions)
df['Thought'] = df['Thought'].apply(remove_sc)
df['Thought'] = df['Thought'].apply(remove_punctuation)
df['Thought'] = df['Thought'].apply(remove_excessive_whitespace)
df['Thought'] = df['Thought'].str.replace(r'\d+', '', regex=True)

df

nan_counts_per_column = df.isna().sum()
print("NaN counts per column:")
print(nan_counts_per_column)

nan_count = df['Thought'].isna().sum()

print(nan_count)



# word_count
df['word_count'] = df['Thought'].apply(lambda x: len(str(x).split()))
# unique_word_count
df['unique_word_count'] = df['Thought'].apply(lambda x: len(set(str(x).split())))
# stop_word_count
df['stop_word_count'] = df['Thought'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))
# url_count
df['url_count'] = df['Thought'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))
# mean_word_length
df['mean_word_length'] = df['Thought'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))
# char_count
df['char_count'] = df['Thought'].apply(lambda x: len(str(x)))
# punctuation_count
df['punctuation_count'] = df['Thought'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))
# hashtag_count
df['hashtag_count'] = df['Thought'].apply(lambda x: len([c for c in str(x) if c == '#']))
# mention_count
df['mention_count'] = df['Thought'].apply(lambda x: len([c for c in str(x) if c == '@']))
# Length of sentence in characters
df['length_of_Thought'] = df['Thought'].apply(lambda x: len(str(x)))
total_digits = (df['Thought'].str.findall(r'\d').apply(len)).sum()
df['length_of_Thought'] = df['length_of_Thought'].astype(str)
df = df[df['length_of_Thought'].apply(len) > 1]

from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk import pos_tag
from nltk.corpus import wordnet
# # Ensure STOPWORDS is defined
STOPWORDS = set(stopwords.words('english'))

# Initialize stemmer and lemmatizer
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

def get_wordnet_pos(treebank_tag):
    """Convert treebank tags to WordNet tags."""
    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN

def preprocess_text(text, use_stemming=False, use_lemmatization=False):
    if use_stemming:
        text = ' '.join([stemmer.stem(word) for word in text.split()])
    if use_lemmatization:
        pos_tags = pos_tag(text.split())
        text = ' '.join([lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags])
    return text

# Apply preprocessing
df['Thought'] = df['Thought'].apply(lambda x: preprocess_text(x, use_stemming=True, use_lemmatization=False))

print("AFTER")
total_word_count = df['word_count'].sum()
print(f"Total Word Count: {total_word_count}")
total_unique_word_count = df['unique_word_count'].sum()
print(f"Total Unique Word Count: {total_unique_word_count}")
total_stop_word_count = df['stop_word_count'].sum()
print(f"Total Stop Word Count: {total_stop_word_count}")
total_url_count = df['url_count'].sum()
print(f"Total URL Count: {total_url_count}")
mean_of_mean_word_length = df['mean_word_length'].mean()
print(f"Mean of Mean Word Length: {mean_of_mean_word_length}")
total_char_count = df['char_count'].sum()
print(f"Total Character Count: {total_char_count}")
total_punctuation_count = df['punctuation_count'].sum()
print(f"Total Punctuation Count: {total_punctuation_count}")
total_hashtag_count = df['hashtag_count'].sum()
print(f"Total Hashtag # Count: {total_hashtag_count}")
total_mention_count = df['mention_count'].sum()
print(f"Total Mention @ Count: {total_mention_count}")
print(f"Total digits Count: {total_digits}")

df

df.describe()

df.isnull().sum()

df = df.dropna()

# Count total NaN values in the entire DataFrame
total_nan_count = df.isna().sum().sum()
print("\nTotal NaN values in the DataFrame:")
print(total_nan_count)



